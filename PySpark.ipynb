{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cGOw5ciXzaB",
        "outputId": "e5f20441-fa07-4b13-f911-06e9a27d67f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=e9a4a84b29f829f2ec89cf5cb6280dc2edfe3585dd5cf6ffbbbaf2bbe8b99795\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "r7dT08-K4fbX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "source": [
        "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "O8LZpF0N4gEt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "source": [
        "type(spark)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "KAlFclOu4gxe",
        "outputId": "a303621c-fa49-4ebe-90c9-2fc949251440"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.session.SparkSession"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.session.SparkSession</b><br/>def __init__(sparkContext: SparkContext, jsparkSession: Optional[JavaObject]=None, options: Dict[str, Any]={})</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py</a>The entry point to programming Spark with the Dataset and DataFrame API.\n",
              "\n",
              "A SparkSession can be used to create :class:`DataFrame`, register :class:`DataFrame` as\n",
              "tables, execute SQL over tables, cache tables, and read parquet files.\n",
              "To create a :class:`SparkSession`, use the following builder pattern:\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              ".. autoattribute:: builder\n",
              "   :annotation:\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Create a Spark session.\n",
              "\n",
              "&gt;&gt;&gt; spark = (\n",
              "...     SparkSession.builder\n",
              "...         .master(&quot;local&quot;)\n",
              "...         .appName(&quot;Word Count&quot;)\n",
              "...         .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)\n",
              "...         .getOrCreate()\n",
              "... )\n",
              "\n",
              "Create a Spark session with Spark Connect.\n",
              "\n",
              "&gt;&gt;&gt; spark = (\n",
              "...     SparkSession.builder\n",
              "...         .remote(&quot;sc://localhost&quot;)\n",
              "...         .appName(&quot;Word Count&quot;)\n",
              "...         .config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;)\n",
              "...         .getOrCreate()\n",
              "... )  # doctest: +SKIP</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 166);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mavbuheB4g4D",
        "outputId": "f28ee721-53ff-4d21-d459-5c48d3fadac6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Builder',\n",
              " '__annotations__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__enter__',\n",
              " '__eq__',\n",
              " '__exit__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_activeSession',\n",
              " '_convert_from_pandas',\n",
              " '_createFromLocal',\n",
              " '_createFromRDD',\n",
              " '_create_dataframe',\n",
              " '_create_from_pandas_with_arrow',\n",
              " '_create_shell_session',\n",
              " '_getActiveSessionOrCreate',\n",
              " '_get_numpy_record_dtype',\n",
              " '_inferSchema',\n",
              " '_inferSchemaFromList',\n",
              " '_instantiatedSession',\n",
              " '_jconf',\n",
              " '_jsc',\n",
              " '_jsparkSession',\n",
              " '_jvm',\n",
              " '_repr_html_',\n",
              " '_sc',\n",
              " 'active',\n",
              " 'addArtifact',\n",
              " 'addArtifacts',\n",
              " 'addTag',\n",
              " 'builder',\n",
              " 'catalog',\n",
              " 'clearTags',\n",
              " 'client',\n",
              " 'conf',\n",
              " 'copyFromLocalToFs',\n",
              " 'createDataFrame',\n",
              " 'getActiveSession',\n",
              " 'getTags',\n",
              " 'interruptAll',\n",
              " 'interruptOperation',\n",
              " 'interruptTag',\n",
              " 'newSession',\n",
              " 'range',\n",
              " 'read',\n",
              " 'readStream',\n",
              " 'removeTag',\n",
              " 'sparkContext',\n",
              " 'sql',\n",
              " 'stop',\n",
              " 'streams',\n",
              " 'table',\n",
              " 'udf',\n",
              " 'udtf',\n",
              " 'version']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(spark.createDataFrame)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GucHYpUr46KF",
        "outputId": "0ea3c418-d972-4a66-ca11-fed0f556b51b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method createDataFrame in module pyspark.sql.session:\n",
            "\n",
            "createDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n",
            "    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n",
            "    or a :class:`numpy.ndarray`.\n",
            "    \n",
            "    .. versionadded:: 2.0.0\n",
            "    \n",
            "    .. versionchanged:: 3.4.0\n",
            "        Supports Spark Connect.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    data : :class:`RDD` or iterable\n",
            "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
            "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n",
            "        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n",
            "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
            "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
            "        column names, default is None. The data type string format equals to\n",
            "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
            "        omit the ``struct<>``.\n",
            "    \n",
            "        When ``schema`` is a list of column names, the type of each column\n",
            "        will be inferred from ``data``.\n",
            "    \n",
            "        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
            "        from ``data``, which should be an RDD of either :class:`Row`,\n",
            "        :class:`namedtuple`, or :class:`dict`.\n",
            "    \n",
            "        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n",
            "        match the real data, or an exception will be thrown at runtime. If the given schema is\n",
            "        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
            "        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n",
            "        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n",
            "        later.\n",
            "    samplingRatio : float, optional\n",
            "        the sample ratio of rows used for inferring. The first few rows will be used\n",
            "        if ``samplingRatio`` is ``None``.\n",
            "    verifySchema : bool, optional\n",
            "        verify data types of every row against schema. Enabled by default.\n",
            "    \n",
            "        .. versionadded:: 2.1.0\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    :class:`DataFrame`\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    Create a DataFrame from a list of tuples.\n",
            "    \n",
            "    >>> spark.createDataFrame([('Alice', 1)]).show()\n",
            "    +-----+---+\n",
            "    |   _1| _2|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create a DataFrame from a list of dictionaries.\n",
            "    \n",
            "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
            "    >>> spark.createDataFrame(d).show()\n",
            "    +---+-----+\n",
            "    |age| name|\n",
            "    +---+-----+\n",
            "    |  1|Alice|\n",
            "    +---+-----+\n",
            "    \n",
            "    Create a DataFrame with column names specified.\n",
            "    \n",
            "    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create a DataFrame with the explicit schema specified.\n",
            "    \n",
            "    >>> from pyspark.sql.types import *\n",
            "    >>> schema = StructType([\n",
            "    ...    StructField(\"name\", StringType(), True),\n",
            "    ...    StructField(\"age\", IntegerType(), True)])\n",
            "    >>> spark.createDataFrame([('Alice', 1)], schema).show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create a DataFrame with the schema in DDL formatted string.\n",
            "    \n",
            "    >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create an empty DataFrame.\n",
            "    When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n",
            "    as the DataFrame lacks data from which the schema can be inferred.\n",
            "    \n",
            "    >>> spark.createDataFrame([], \"name: string, age: int\").show()\n",
            "    +----+---+\n",
            "    |name|age|\n",
            "    +----+---+\n",
            "    +----+---+\n",
            "    \n",
            "    Create a DataFrame from Row objects.\n",
            "    \n",
            "    >>> from pyspark.sql import Row\n",
            "    >>> Person = Row('name', 'age')\n",
            "    >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n",
            "    >>> df.show()\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    \n",
            "    Create a DataFrame from a pandas DataFrame.\n",
            "    \n",
            "    >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n",
            "    +-----+---+\n",
            "    | name|age|\n",
            "    +-----+---+\n",
            "    |Alice|  1|\n",
            "    +-----+---+\n",
            "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
            "    +---+---+\n",
            "    |  0|  1|\n",
            "    +---+---+\n",
            "    |  1|  2|\n",
            "    +---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "\n",
        "data = [(1,'Nayeer'),(2,'Naushad')]\n",
        "schema=['id','name']\n",
        "\n",
        "df=spark.createDataFrame(data=data,schema=schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue5mYhkL8b9L",
        "outputId": "85cda574-b072-4bdd-c9b1-fc4c8e8b9557"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1| Nayeer|\n",
            "|  2|Naushad|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "\n",
        "data = [(1,'Nayeer'),(2,'Naushad')]\n",
        "StructType= ([StructField(name='id',dataType=IntegerType()),\n",
        "              StructField(name='id',dataType=IntegerType())])\n",
        "schema=['id','name']\n",
        "df=spark.createDataFrame(data=data,schema=schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B62J4O6I_NY5",
        "outputId": "5127afbd-2143-4539-e110-5a5b2bb5bcfe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1| Nayeer|\n",
            "|  2|Naushad|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [{'id':1,'name':'Nayeer'},\n",
        "        {'id':2,'name':'Naushad'}]\n",
        "\n",
        "df= spark.createDataFrame(data=data,schema=schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RZC2dU5A0QK",
        "outputId": "5a3e81a5-54e7-4f1d-dde1-66e141bc0af5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1| Nayeer|\n",
            "|  2|Naushad|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ##Read CSV files\n",
        "# df = spark.read.csv(\"dataset_Loan.csv\",header=True)\n",
        "# display(df)\n",
        "# df.printSchema()\n",
        "# df.show()"
      ],
      "metadata": {
        "id": "WdkSJKphBpC9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df= spark.read.format('csv').option(key='header',value=True).load(\"dataset_Loan.csv\")\n",
        "# display(df)"
      ],
      "metadata": {
        "id": "r9cQmPXmXkeJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ##multiple csv file\n",
        "# df = spark.read.csv(['dataset_Loan.csv','spam.csv'],header=True)\n",
        "# display(df)\n",
        "# df.show()\n",
        "# df.printSchema()"
      ],
      "metadata": {
        "id": "sqo8P3G_ZOwk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ##reading All the csv file at once\n",
        "\n",
        "\n",
        "# df = spark.read.csv(path='/content/CSV',header=True)\n",
        "# display(df)"
      ],
      "metadata": {
        "id": "wBPHRZabaZHR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WriteDFtoCSV\n"
      ],
      "metadata": {
        "id": "egy0AJWtF2ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "\n",
        "data = [(1,\"nayeer\"),(2,\"naushad\")]\n",
        "schema = ['id','name']\n",
        "\n",
        "spark.createDataFrame(data=data,schema=schema)\n",
        "display(df)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "vsdqSfUZb3ak",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "0854aaa2-1b77-4fe2-ed02-a7377c71923b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1| Nayeer|\n",
            "|  2|Naushad|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.option(\"header\", True).csv(\"/content/gth\")"
      ],
      "metadata": {
        "id": "2tfXWLhNGEQD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.options(header=\"True\",delimiter=',').csv(\"/content/gas\")"
      ],
      "metadata": {
        "id": "FXHod7nSGZQL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=spark.read.csv(path=\"/content/gth\",header=True)\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SqwS22sHpGL",
        "outputId": "bddadb58-76f5-451e-8505-fa05986eca39"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  2|Naushad|\n",
            "|  1| Nayeer|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#overwrite\n",
        "df.write.csv(path=\"/content/gth\",header=True,mode='overwrite')"
      ],
      "metadata": {
        "id": "mtLDBDazJxa4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=spark.read.csv(path=\"/content/gth\",header=True)\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ6R670NK-lo",
        "outputId": "756ddc67-8a31-45af-9728-89a11242d290"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  2|Naushad|\n",
            "|  1| Nayeer|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#append\n",
        "df.write.csv(path=\"/content/gth\",header=True,mode='append')"
      ],
      "metadata": {
        "id": "8U7sI2jXLDGh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.csv(path=\"/content/gth\",header=True)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Erk7pNTgLf0a",
        "outputId": "91457bca-447b-49b4-8dfd-82c4b6513df7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  2|Naushad|\n",
            "|  2|Naushad|\n",
            "|  1| Nayeer|\n",
            "|  1| Nayeer|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read JSON to DF\n",
        "# help(spark.read)\n",
        "# help(spark.read.json)"
      ],
      "metadata": {
        "id": "2YYl9tg9Lqtq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.json(path=\"/content/sample1.json\")\n",
        "df.printSchema()\n",
        "df.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "gwLwyYAARERz",
        "outputId": "51b9eafb-a814-46f6-b80d-97d8558a5c87"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _corrupt_record: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.show of DataFrame[_corrupt_record: string]>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame.show</b><br/>def show(n: int=20, truncate: Union[bool, int]=True, vertical: bool=False) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py</a>Prints the first ``n`` rows to the console.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "n : int, optional\n",
              "    Number of rows to show.\n",
              "truncate : bool or int, optional\n",
              "    If set to ``True``, truncate strings longer than 20 chars by default.\n",
              "    If set to a number greater than one, truncates long strings to length ``truncate``\n",
              "    and align cells right.\n",
              "vertical : bool, optional\n",
              "    If set to ``True``, print output rows vertically (one line\n",
              "    per column value).\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; df = spark.createDataFrame([\n",
              "...     (14, &quot;Tom&quot;), (23, &quot;Alice&quot;), (16, &quot;Bob&quot;)], [&quot;age&quot;, &quot;name&quot;])\n",
              "\n",
              "Show only top 2 rows.\n",
              "\n",
              "&gt;&gt;&gt; df.show(2)\n",
              "+---+-----+\n",
              "|age| name|\n",
              "+---+-----+\n",
              "| 14|  Tom|\n",
              "| 23|Alice|\n",
              "+---+-----+\n",
              "only showing top 2 rows\n",
              "\n",
              "Show :class:`DataFrame` where the maximum number of characters is 3.\n",
              "\n",
              "&gt;&gt;&gt; df.show(truncate=3)\n",
              "+---+----+\n",
              "|age|name|\n",
              "+---+----+\n",
              "| 14| Tom|\n",
              "| 23| Ali|\n",
              "| 16| Bob|\n",
              "+---+----+\n",
              "\n",
              "Show :class:`DataFrame` vertically.\n",
              "\n",
              "&gt;&gt;&gt; df.show(vertical=True)\n",
              "-RECORD 0-----\n",
              "age  | 14\n",
              "name | Tom\n",
              "-RECORD 1-----\n",
              "age  | 23\n",
              "name | Alice\n",
              "-RECORD 2-----\n",
              "age  | 16\n",
              "name | Bob</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 885);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df=spark.read.json(path=\"/content/data.json\",multiLine=True)\n",
        "# # df.printSchema()\n",
        "# df.show()"
      ],
      "metadata": {
        "id": "7ATHvCDCScA-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df=spark.read.json(path=['/content/data.json','/content/sample2.json'],multiLine=True)\n",
        "# # df.printSchema()\n",
        "# df.show()"
      ],
      "metadata": {
        "id": "3JAXROO9TeoA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df=spark.read.json(path='/content/*.json')\n",
        "# df.printSchema()\n",
        "# df.show()"
      ],
      "metadata": {
        "id": "JxSKcNZvUW8J"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#write DF to JSON\n",
        "data = [(1,'nayeer'),(2,'naushad')]\n",
        "schema=['id','name']\n",
        "df=spark.createDataFrame(data=data,schema=schema)\n",
        "display(df)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "M39PwrmKVA05",
        "outputId": "63c8bc5d-120f-4135-9494-cd767c18c404"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1| nayeer|\n",
            "|  2|naushad|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#storing as as json\n",
        "df.write.json('/content/neww.json')"
      ],
      "metadata": {
        "id": "-dHrZ3kweHzd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.json('/content/neww.json')\n",
        "df.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "dUJfIDvTeuM-",
        "outputId": "67e69d84-c1ae-40d7-9e72-eedfd6e1dacd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.show of DataFrame[id: bigint, name: string]>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame.show</b><br/>def show(n: int=20, truncate: Union[bool, int]=True, vertical: bool=False) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py</a>Prints the first ``n`` rows to the console.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "n : int, optional\n",
              "    Number of rows to show.\n",
              "truncate : bool or int, optional\n",
              "    If set to ``True``, truncate strings longer than 20 chars by default.\n",
              "    If set to a number greater than one, truncates long strings to length ``truncate``\n",
              "    and align cells right.\n",
              "vertical : bool, optional\n",
              "    If set to ``True``, print output rows vertically (one line\n",
              "    per column value).\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; df = spark.createDataFrame([\n",
              "...     (14, &quot;Tom&quot;), (23, &quot;Alice&quot;), (16, &quot;Bob&quot;)], [&quot;age&quot;, &quot;name&quot;])\n",
              "\n",
              "Show only top 2 rows.\n",
              "\n",
              "&gt;&gt;&gt; df.show(2)\n",
              "+---+-----+\n",
              "|age| name|\n",
              "+---+-----+\n",
              "| 14|  Tom|\n",
              "| 23|Alice|\n",
              "+---+-----+\n",
              "only showing top 2 rows\n",
              "\n",
              "Show :class:`DataFrame` where the maximum number of characters is 3.\n",
              "\n",
              "&gt;&gt;&gt; df.show(truncate=3)\n",
              "+---+----+\n",
              "|age|name|\n",
              "+---+----+\n",
              "| 14| Tom|\n",
              "| 23| Ali|\n",
              "| 16| Bob|\n",
              "+---+----+\n",
              "\n",
              "Show :class:`DataFrame` vertically.\n",
              "\n",
              "&gt;&gt;&gt; df.show(vertical=True)\n",
              "-RECORD 0-----\n",
              "age  | 14\n",
              "name | Tom\n",
              "-RECORD 1-----\n",
              "age  | 23\n",
              "name | Alice\n",
              "-RECORD 2-----\n",
              "age  | 16\n",
              "name | Bob</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 885);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.json('/content/neww.json',mode='ignore')"
      ],
      "metadata": {
        "id": "f25pHDiLfDWl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.json('/content/neww.json')\n",
        "display(df)\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "gOFzMlu9g-2w",
        "outputId": "3fb6053c-709d-4407-b718-644ebe2884b4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  2|naushad|\n",
            "|  1| nayeer|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read Parquet file into DataFrame using Pyspark\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n"
      ],
      "metadata": {
        "id": "OvTwlS3VqOnf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyarrow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHAuNlcOqhKT",
        "outputId": "468a2ac8-3913-4d0b-fac5-a0c0fe43dcc7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (14.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read single parquet file\n",
        "df= spark.read.parquet('/content/MT cars.parquet')\n",
        "df.show()\n",
        "print(df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSNHeosppA38",
        "outputId": "45794771-8779-46e5-eee7-c5a0ad4bad85"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "|              model| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "|          Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
            "|      Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
            "|         Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
            "|     Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|\n",
            "|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|\n",
            "|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|\n",
            "|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|\n",
            "|          Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|\n",
            "|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|\n",
            "|           Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|\n",
            "|          Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|\n",
            "|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|\n",
            "|         Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|\n",
            "|        Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|\n",
            "| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|\n",
            "|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|\n",
            "|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|\n",
            "|           Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|\n",
            "|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|\n",
            "|     Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|\n",
            "+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+\n",
            "only showing top 20 rows\n",
            "\n",
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read all the parquet file\n",
        "df= spark.read.parquet('/content/*.parquet')\n",
        "df.show()\n",
        "print(df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRYVBOBvqEtV",
        "outputId": "369fd452-b0b7-43a9-9c89-47b318981090"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal.length|sepal.width|petal.length|petal.width|variety|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "|        NULL|       NULL|        NULL|       NULL|   NULL|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 20 rows\n",
            "\n",
            "182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#write DF to Parquet\n",
        "data=[(1,'Nayeer'),(2,'Naushad')]\n",
        "schema=['id','name']\n",
        "\n",
        "df = spark.createDataFrame(data = data , schema = schema)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "PNHwelOFrMGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0328fce1-e2cc-4699-add8-cce534925fa8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1| Nayeer|\n",
            "|  2|Naushad|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#storing df into parquet\n",
        "df.write.parquet('/content/parquetop1')"
      ],
      "metadata": {
        "id": "uKF-ztxoqKAI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1= spark.read.parquet('/content/parquetop1')\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZx1loqorAOu",
        "outputId": "6729479c-8baf-4625-e6df-0fa427c4ea66"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  2|Naushad|\n",
            "|  1| Nayeer|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#storing df into parquet\n",
        "df.write.parquet('/content/parquetop1',mode='append')"
      ],
      "metadata": {
        "id": "bh5g4pn4rMEz"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1= spark.read.parquet('/content/parquetop1')\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_Jr6EDWreyM",
        "outputId": "76765524-f4f1-4f1b-8b00-2f11266d1e50"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  2|Naushad|\n",
            "|  2|Naushad|\n",
            "|  1| Nayeer|\n",
            "|  1| Nayeer|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.parquet('/content/parquetop1',mode='overwrite')"
      ],
      "metadata": {
        "id": "qxhf67Zarp8M"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1= spark.read.parquet('/content/parquetop1')\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kva9OOifr1bM",
        "outputId": "9eca82fd-a716-48a2-92de-1c26fd975064"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  2|Naushad|\n",
            "|  1| Nayeer|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.parquet('/content/parquetop1',mode='ignore')"
      ],
      "metadata": {
        "id": "eUzY8t60sB-T"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1= spark.read.parquet('/content/parquetop1')\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88i0Jwz8r5FO",
        "outputId": "bba3877b-1f74-4b53-9586-5525ac7315b4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  2|Naushad|\n",
            "|  1| Nayeer|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#showNoteBook\n",
        "data=[(1,'Kingnvksnvnsvnvnskvnksnvksnvksvnknvkvnksnvk'),\n",
        "      (2,'Princsggssvvsvsvsvse'),\n",
        "      (3,'Hitmansvsvsrvsrv'),\n",
        "      (4,'Jaddusrgsrggsrjgsrjogrgosrisrjg')]\n",
        "\n",
        "schema=['id','name']\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.show(truncate=False) # for showing full data value\n",
        "df.show(truncate=2)\n",
        "df.show(truncate=False,vertical=True)\n",
        "df.show(n=2,truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1O38fFesEgc",
        "outputId": "7ef341c6-9669-4556-a088-e0b10d3cc31e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|                name|\n",
            "+---+--------------------+\n",
            "|  1|Kingnvksnvnsvnvns...|\n",
            "|  2|Princsggssvvsvsvsvse|\n",
            "|  3|    Hitmansvsvsrvsrv|\n",
            "|  4|Jaddusrgsrggsrjgs...|\n",
            "+---+--------------------+\n",
            "\n",
            "+---+-------------------------------------------+\n",
            "|id |name                                       |\n",
            "+---+-------------------------------------------+\n",
            "|1  |Kingnvksnvnsvnvnskvnksnvksnvksvnknvkvnksnvk|\n",
            "|2  |Princsggssvvsvsvsvse                       |\n",
            "|3  |Hitmansvsvsrvsrv                           |\n",
            "|4  |Jaddusrgsrggsrjgsrjogrgosrisrjg            |\n",
            "+---+-------------------------------------------+\n",
            "\n",
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1|  Ki|\n",
            "|  2|  Pr|\n",
            "|  3|  Hi|\n",
            "|  4|  Ja|\n",
            "+---+----+\n",
            "\n",
            "-RECORD 0-------------------------------------------\n",
            " id   | 1                                           \n",
            " name | Kingnvksnvnsvnvnskvnksnvksnvksvnknvkvnksnvk \n",
            "-RECORD 1-------------------------------------------\n",
            " id   | 2                                           \n",
            " name | Princsggssvvsvsvsvse                        \n",
            "-RECORD 2-------------------------------------------\n",
            " id   | 3                                           \n",
            " name | Hitmansvsvsrvsrv                            \n",
            "-RECORD 3-------------------------------------------\n",
            " id   | 4                                           \n",
            " name | Jaddusrgsrggsrjgsrjogrgosrisrjg             \n",
            "\n",
            "+---+-------------------------------------------+\n",
            "|id |name                                       |\n",
            "+---+-------------------------------------------+\n",
            "|1  |Kingnvksnvnsvnvnskvnksnvksnvksvnknvkvnksnvk|\n",
            "|2  |Princsggssvvsvsvsvse                       |\n",
            "+---+-------------------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#showNoteBook\n",
        "from pyspark.sql.functions import col\n",
        "data=[(1,'786'),\n",
        "      (2,'Princsggssvvsvsvsvse'),\n",
        "      (3,'Hitmansvsvsrvsrv'),\n",
        "      (4,'Jaddusrgsrggsrjgsrjogrgosrisrjg')]\n",
        "\n",
        "columns=['id','name']\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "\n",
        "df.show()\n",
        "df1=df.withColumn(colName='name',col=col('name').cast('Integer'))\n",
        "\n",
        "df1.show()\n",
        "df1.printSchema()\n",
        "\n",
        "\n",
        "df2=df1.withColumn('name',col('name')*2)\n",
        "df2.show()\n",
        "df2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0rBkUlWtaTG",
        "outputId": "5d4463ec-b1be-421d-e41d-cd8e31aefcbe"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|                name|\n",
            "+---+--------------------+\n",
            "|  1|                 786|\n",
            "|  2|Princsggssvvsvsvsvse|\n",
            "|  3|    Hitmansvsvsrvsrv|\n",
            "|  4|Jaddusrgsrggsrjgs...|\n",
            "+---+--------------------+\n",
            "\n",
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1| 786|\n",
            "|  2|NULL|\n",
            "|  3|NULL|\n",
            "|  4|NULL|\n",
            "+---+----+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: integer (nullable = true)\n",
            "\n",
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1|1572|\n",
            "|  2|NULL|\n",
            "|  3|NULL|\n",
            "|  4|NULL|\n",
            "+---+----+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "df3 = df2.withColumn('salary',lit('2000'))  ## lit will add new column\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_QESaTzyW21",
        "outputId": "105a0e7b-2912-46c3-e5b9-2f3f32c09d18"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+\n",
            "| id|name|salary|\n",
            "+---+----+------+\n",
            "|  1|1572|  2000|\n",
            "|  2|NULL|  2000|\n",
            "|  3|NULL|  2000|\n",
            "|  4|NULL|  2000|\n",
            "+---+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = df3.withColumn('copiedsalary',col('salary'))  ## to make duplicate\n",
        "df4.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTpxyc8i0qTg",
        "outputId": "71bc0c72-dc5d-4ec5-ee10-3dd5a5a91ce3"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+------------+\n",
            "| id|name|salary|copiedsalary|\n",
            "+---+----+------+------------+\n",
            "|  1|1572|  2000|        2000|\n",
            "|  2|NULL|  2000|        2000|\n",
            "|  3|NULL|  2000|        2000|\n",
            "|  4|NULL|  2000|        2000|\n",
            "+---+----+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#renaming using withColumnRename()\n",
        "data=[(1,'Kingnvksnvnsvnvnskvnksnvksnvksvnknvkvnksnvk'),\n",
        "      (2,'Princsggssvvsvsvsvse'),\n",
        "      (3,'Hitmansvsvsrvsrv'),\n",
        "      (4,'Jaddusrgsrggsrjgsrjogrgosrisrjg')]\n",
        "\n",
        "schema=['id','name']\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "\n",
        "df1=df.withColumnRenamed('name','names')\n",
        "df1.show()\n"
      ],
      "metadata": {
        "id": "mem0u6F51GaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a83d14-f869-4f19-cf29-b6cb965c8a8c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|               names|\n",
            "+---+--------------------+\n",
            "|  1|Kingnvksnvnsvnvns...|\n",
            "|  2|Princsggssvvsvsvsvse|\n",
            "|  3|    Hitmansvsvsrvsrv|\n",
            "|  4|Jaddusrgsrggsrjgs...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#structType structField\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "data= [(1,'Nayeer',4500),(2,'Naushad',4000)]\n",
        "\n",
        "schema=StructType([\\\n",
        "                   StructField(name='id',dataType=IntegerType()),\\\n",
        "                   StructField(name='name',dataType=StringType()),\\\n",
        "                   StructField(name='Salary',dataType=IntegerType())\\\n",
        "                   ])\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPX4UosAsjRb",
        "outputId": "a025ae50-04cc-4fee-c737-d820a31b6af4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+\n",
            "| id|   name|Salary|\n",
            "+---+-------+------+\n",
            "|  1| Nayeer|  4500|\n",
            "|  2|Naushad|  4000|\n",
            "+---+-------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#structType structField\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "data= [(1,('Nay','King'),4500),(2,('Eer','Prince'),4000)]\n",
        "\n",
        "structName=StructType([\\\n",
        "                       StructField('firstname',StringType()),\\\n",
        "                       StructField('lastname',StringType())])\n",
        "\n",
        "schema=StructType([\\\n",
        "                   StructField(name='id',dataType=IntegerType()),\\\n",
        "                   StructField(name='name',dataType=structName),\\\n",
        "                   StructField(name='Salary',dataType=IntegerType())\\\n",
        "                   ])\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhLQ50D9tsmU",
        "outputId": "8dce2b6f-6f72-43e1-ee41-d08f06b8cd92"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+------+\n",
            "| id|         name|Salary|\n",
            "+---+-------------+------+\n",
            "|  1|  {Nay, King}|  4500|\n",
            "|  2|{Eer, Prince}|  4000|\n",
            "+---+-------------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#array type notebook\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType\n",
        "\n",
        "data = [('abc',[1,2]),('def',[3,4]),('ghi',[5,6]),('adada',[11,22]),('abdafadfadfc',[111,112])]\n",
        "# schema=['id','numbers']\n",
        "schema=StructType([\\\n",
        "                       StructField('id',StringType()),\\\n",
        "                       StructField('numbers',ArrayType(IntegerType()))])\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "df.withColumn('firstNumber',df.numbers[0]).show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4mWtj0E6wRgA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0353e66-8bc3-494b-b6e1-a01dae7901c4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+\n",
            "|          id|   numbers|\n",
            "+------------+----------+\n",
            "|         abc|    [1, 2]|\n",
            "|         def|    [3, 4]|\n",
            "|         ghi|    [5, 6]|\n",
            "|       adada|  [11, 22]|\n",
            "|abdafadfadfc|[111, 112]|\n",
            "+------------+----------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- numbers: array (nullable = true)\n",
            " |    |-- element: integer (containsNull = true)\n",
            "\n",
            "+------------+----------+-----------+\n",
            "|          id|   numbers|firstNumber|\n",
            "+------------+----------+-----------+\n",
            "|         abc|    [1, 2]|          1|\n",
            "|         def|    [3, 4]|          3|\n",
            "|         ghi|    [5, 6]|          5|\n",
            "|       adada|  [11, 22]|         11|\n",
            "|abdafadfadfc|[111, 112]|        111|\n",
            "+------------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,array\n",
        "df=spark.createDataFrame(\n",
        "    [(33,44),(55,66)],[\"num1\",\"num2\"]\n",
        ")\n",
        "df.show()\n",
        "\n",
        "df.withColumn(\"numbers\",array(df.num1,df.num2)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZWVDeXd3WlS",
        "outputId": "413e01aa-b89a-4f91-8b8c-802f8aa5c0f5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+\n",
            "|num1|num2|\n",
            "+----+----+\n",
            "|  33|  44|\n",
            "|  55|  66|\n",
            "+----+----+\n",
            "\n",
            "+----+----+--------+\n",
            "|num1|num2| numbers|\n",
            "+----+----+--------+\n",
            "|  33|  44|[33, 44]|\n",
            "|  55|  66|[55, 66]|\n",
            "+----+----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Use explode() function to create a new row for each element in the given array column.\n",
        "from pyspark.sql.types import StructType , StructField , StringType , IntegerType , ArrayType\n",
        "from pyspark.sql.functions import col,explode\n",
        "\n",
        "data = [('abc',[1,2]),('def',[3,4]),('ghi',[5,6]),('adada',[11,22]),('abdafadfadfc',[111,112])]\n",
        "# schema=['id','numbers']\n",
        "schema=StructType([\\\n",
        "                       StructField('id',StringType()),\\\n",
        "                       StructField('numbers',ArrayType(IntegerType()))])\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df1=df.withColumn('explodedCol',explode(col('numbers'))).select('id','explodedCol')\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hwXCzwq31V0",
        "outputId": "d65c2a3d-f687-4bbb-e472-8f5343a541fd"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+\n",
            "|          id|   numbers|\n",
            "+------------+----------+\n",
            "|         abc|    [1, 2]|\n",
            "|         def|    [3, 4]|\n",
            "|         ghi|    [5, 6]|\n",
            "|       adada|  [11, 22]|\n",
            "|abdafadfadfc|[111, 112]|\n",
            "+------------+----------+\n",
            "\n",
            "+------------+-----------+\n",
            "|          id|explodedCol|\n",
            "+------------+-----------+\n",
            "|         abc|          1|\n",
            "|         abc|          2|\n",
            "|         def|          3|\n",
            "|         def|          4|\n",
            "|         ghi|          5|\n",
            "|         ghi|          6|\n",
            "|       adada|         11|\n",
            "|       adada|         22|\n",
            "|abdafadfadfc|        111|\n",
            "|abdafadfadfc|        112|\n",
            "+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Use split() function to create a new row for each element in the given array column.\n",
        "# from pyspark.sql.types import StructType , StructField , StringType , IntegerType , ArrayType\n",
        "from pyspark.sql.functions import split\n",
        "data=[(1,'Nayeer','dyna,cae,wew'),(2,'Naushad','cloud,py,aws')]\n",
        "schema=['id','name','skills']\n",
        "\n",
        "df = spark.createDataFrame(data  , schema)\n",
        "df=df.withColumn('skills',split('skills',','))\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NPO9XxQ6qbG",
        "outputId": "c5861353-d73d-4c90-9c35-5314380c2f6a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----------------+\n",
            "| id|   name|          skills|\n",
            "+---+-------+----------------+\n",
            "|  1| Nayeer|[dyna, cae, wew]|\n",
            "|  2|Naushad|[cloud, py, aws]|\n",
            "+---+-------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Use array() function to create a new row for each element in the given array column.\n",
        "# from pyspark.sql.types import StructType , StructField , StringType , IntegerType , ArrayType\n",
        "from pyspark.sql.functions import split , array , col\n",
        "data=[(1,'Nayeer','dyna','cae'),(2,'Naushad','cloud','py')]\n",
        "schema=['id','name','skillsf','skillss']\n",
        "\n",
        "df = spark.createDataFrame(data  , schema)\n",
        "df.show()\n",
        "df1=df.withColumn('skills',array(col('skillsf'),col('skillss')))\n",
        "\n",
        "df1.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny_2lwh_71Y7",
        "outputId": "1c9d7e44-ac89-4beb-de25-d9d4040626c1"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-------+-------+\n",
            "| id|   name|skillsf|skillss|\n",
            "+---+-------+-------+-------+\n",
            "|  1| Nayeer|   dyna|    cae|\n",
            "|  2|Naushad|  cloud|     py|\n",
            "+---+-------+-------+-------+\n",
            "\n",
            "+---+-------+-------+-------+-----------+\n",
            "| id|   name|skillsf|skillss|     skills|\n",
            "+---+-------+-------+-------+-----------+\n",
            "|  1| Nayeer|   dyna|    cae|[dyna, cae]|\n",
            "|  2|Naushad|  cloud|     py|[cloud, py]|\n",
            "+---+-------+-------+-------+-----------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skillsf: string (nullable = true)\n",
            " |-- skillss: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Use arraycontains() function to create a new row for each element in the given array column.\n",
        "# from pyspark.sql.types import StructType , StructField , StringType , IntegerType , ArrayType\n",
        "from pyspark.sql.functions import split , array , col , array_contains\n",
        "data=[(1,'Nayeer',['dyna','cae']),(2,'Naushad',['cloud','py'])]\n",
        "schema=['id','name','skills']\n",
        "\n",
        "df = spark.createDataFrame(data  , schema)\n",
        "df.show()\n",
        "df1=df.withColumn('hasJavaSkill',array_contains(col('skills'),'java'))\n",
        "\n",
        "df1.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8uJgkoJ9VX7",
        "outputId": "49f5e9d3-8a38-4255-d5f0-4de5ae8daaa2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-----------+\n",
            "| id|   name|     skills|\n",
            "+---+-------+-----------+\n",
            "|  1| Nayeer|[dyna, cae]|\n",
            "|  2|Naushad|[cloud, py]|\n",
            "+---+-------+-----------+\n",
            "\n",
            "+---+-------+-----------+------------+\n",
            "| id|   name|     skills|hasJavaSkill|\n",
            "+---+-------+-----------+------------+\n",
            "|  1| Nayeer|[dyna, cae]|       false|\n",
            "|  2|Naushad|[cloud, py]|       false|\n",
            "+---+-------+-----------+------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#- PySpark MapType is used to represent map key-value pair similar to python Dictionary (Dict).\n",
        "data=[('nayeer',{'hair':'black','eye':'brown'}),('naushad',{'hair':'brown','eye':'brown'})]\n",
        "schema=['name','properties']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNVlC4Wt98UN",
        "outputId": "1e126653-7ea1-4587-8908-7f6397f07650"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+\n",
            "|   name|          properties|\n",
            "+-------+--------------------+\n",
            "| nayeer|{eye -> brown, ha...|\n",
            "|naushad|{eye -> brown, ha...|\n",
            "+-------+--------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField,StringType, MapType\n",
        "\n",
        "\n",
        "data=[('nayeer',{'hair':'black','eye':'brown'}),('naushad',{'hair':'brown','eye':'brown'})]\n",
        "schema=StructType([\\\n",
        "                   StructField('name',StringType()),\\\n",
        "                   StructField('properties',MapType(StringType(),StringType()))\\\n",
        "                   ])\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show(truncate=False)\n",
        "display(df)\n",
        "df.printSchema()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "ijTf3bR6qFx-",
        "outputId": "b3e59fd1-8601-4003-d429-1fdc39f4cf4c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------------+\n",
            "|name   |properties                   |\n",
            "+-------+-----------------------------+\n",
            "|nayeer |{eye -> brown, hair -> black}|\n",
            "|naushad|{eye -> brown, hair -> brown}|\n",
            "+-------+-----------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[name: string, properties: map<string,string>]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Access maptype elements\n",
        "df=df.withColumn('hair',df.properties['hair'])\n",
        "df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tJ_wq8VrHOn",
        "outputId": "c0ffb614-1fda-400a-edd9-bbec825577aa"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+-----+\n",
            "|   name|          properties| hair|\n",
            "+-------+--------------------+-----+\n",
            "| nayeer|{eye -> brown, ha...|black|\n",
            "|naushad|{eye -> brown, ha...|brown|\n",
            "+-------+--------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#explode\n",
        "from pyspark.sql.functions import explode\n",
        "df.select('name','properties',explode(df.properties)).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nDuZ5OYuHML",
        "outputId": "a11d8a23-ccd7-41f1-e248-0193e4d5f24e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------------+----+-----+\n",
            "|name   |properties                   |key |value|\n",
            "+-------+-----------------------------+----+-----+\n",
            "|nayeer |{eye -> brown, hair -> black}|eye |brown|\n",
            "|nayeer |{eye -> brown, hair -> black}|hair|black|\n",
            "|naushad|{eye -> brown, hair -> brown}|eye |brown|\n",
            "|naushad|{eye -> brown, hair -> brown}|hair|brown|\n",
            "+-------+-----------------------------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#map_keys()\n",
        "#explode\n",
        "from pyspark.sql.functions import map_keys\n",
        "df1=df.withColumn('keys',map_keys(df.properties))\n",
        "df1.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYLtP_EEvVJi",
        "outputId": "913ab84c-66b8-494b-8c88-36a916928a2b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------------+-----+-----------+\n",
            "|name   |properties                   |hair |keys       |\n",
            "+-------+-----------------------------+-----+-----------+\n",
            "|nayeer |{eye -> brown, hair -> black}|black|[eye, hair]|\n",
            "|naushad|{eye -> brown, hair -> brown}|brown|[eye, hair]|\n",
            "+-------+-----------------------------+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##map_keys()\n",
        "#explode\n",
        "from pyspark.sql.functions import map_values\n",
        "df1=df.withColumn('values',map_values(df.properties))\n",
        "df1.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK0ylE-cxmK_",
        "outputId": "49417fb7-fa44-44a8-e6e0-1513ca655975"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------------+-----+--------------+\n",
            "|name   |properties                   |hair |values        |\n",
            "+-------+-----------------------------+-----+--------------+\n",
            "|nayeer |{eye -> brown, hair -> black}|black|[brown, black]|\n",
            "|naushad|{eye -> brown, hair -> brown}|brown|[brown, brown]|\n",
            "+-------+-----------------------------+-----+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pyspark.sql.row which is represented as record row in fdataframe , one\n",
        "# can create a row object by using named argumnets or create a custom row like class,\n",
        "\n",
        "from pyspark.sql import Row\n",
        "row=Row('nayeer',2000)\n",
        "print(row[0]+' '+str(row[1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRd_twt4yeGZ",
        "outputId": "cb5cefc3-fe83-4c15-915b-d90b3043e950"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nayeer 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using named argument\n",
        "from pyspark.sql import Row\n",
        "row=Row(name='Nayeer',salary=2000)\n",
        "print(row.name+' '+str(row.salary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEl1k9Pu00GH",
        "outputId": "f61c35ac-0c18-42f5-831a-9bb4b576a0b9"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nayeer 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we can also create row like this\n",
        "person=Row('name','salary')\n",
        "p1=person('nayeer',20000)\n",
        "p2=person('naushad',20002)\n",
        "\n",
        "print(p1.name+' '+p2.name)\n",
        "print(str(p1.salary)+' '+str(p2.salary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BP_8fcV-1v90",
        "outputId": "1aafe62c-928c-4824-8224-5723f76f07fb"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nayeer naushad\n",
            "20000 20002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data  in dataframe represents like this\n",
        "data=[p1,p2]\n",
        "df=spark.createDataFrame(data)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlFYggmH33IQ",
        "outputId": "4ee262b8-155a-4905-e14e-4718448b66df"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|   name|salary|\n",
            "+-------+------+\n",
            "| nayeer| 20000|\n",
            "|naushad| 20002|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Column class in PySpark | pyspark.sql.Column\n",
        "from pyspark.sql.functions import lit\n",
        "col1=lit(\"abd\")\n",
        "print(type(col1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DP89M9gYR7I",
        "outputId": "22eefd6c-5464-47d7-b095-b633f3fb1634"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.column.Column'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "data=[('nayeer','male',20000),('naushad','male',300000)]\n",
        "schema=['name','gender','salary']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL7We7TDcPgI",
        "outputId": "6adb27a3-a87a-46cb-9d83-c9a18dba523b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------+\n",
            "|   name|gender|salary|\n",
            "+-------+------+------+\n",
            "| nayeer|  male| 20000|\n",
            "|naushad|  male|300000|\n",
            "+-------+------+------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "df.select(df.gender).show()\n",
        "df.select(df['gender']).show()\n",
        "df.select(col('gender')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzBtWF8JZyKZ",
        "outputId": "2487d16c-880c-4fa5-ac65-308bbed5c044"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|  male|\n",
            "|  male|\n",
            "+------+\n",
            "\n",
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|  male|\n",
            "|  male|\n",
            "+------+\n",
            "\n",
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|  male|\n",
            "|  male|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql.functions import col\n",
        "# df.select(df.properties.hair).show()\n",
        "# df.select(df['properties.hair']).show()\n",
        "# df.select(col('properties.hair')).show()\n"
      ],
      "metadata": {
        "id": "HzAjb_DQbWP7"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "data=[(1,'nayeer','male'),(2,'naca','feMale'),(3,'nadcadca','unknown')]\n",
        "schema=['id','name','gender']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "df1=df.select(df.id,df.name,when(condition=df.gender=='male',value='male')\\\n",
        "              .when (condition=df.gender=='feMale',value='female')\\\n",
        "              .otherwise('unknown').alias('gender'))\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-EXooWwbl1K",
        "outputId": "e146d70d-1a68-4ea4-b358-2ee4452d641b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+-------+\n",
            "| id|    name| gender|\n",
            "+---+--------+-------+\n",
            "|  1|  nayeer|   male|\n",
            "|  2|    naca| feMale|\n",
            "|  3|nadcadca|unknown|\n",
            "+---+--------+-------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n",
            "+---+--------+-------+\n",
            "| id|    name| gender|\n",
            "+---+--------+-------+\n",
            "|  1|  nayeer|   male|\n",
            "|  2|    naca| female|\n",
            "|  3|nadcadca|unknown|\n",
            "+---+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#alias\n",
        "from pyspark.sql.functions import when\n",
        "data=[(1,'nayeer','male'),(2,'naca','feMale'),(3,'nadcadca','unknown')]\n",
        "schema=['id','name','gender']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.select(df.id.alias('emp_id'),df.name.alias('emp_name')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzA83oYuZxWE",
        "outputId": "7d963402-f58a-426c-e5d8-9459bc12eb0f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+\n",
            "|emp_id|emp_name|\n",
            "+------+--------+\n",
            "|     1|  nayeer|\n",
            "|     2|    naca|\n",
            "|     3|nadcadca|\n",
            "+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#asc() desc()\n",
        "from pyspark.sql.functions import when\n",
        "data=[(1,'nayeer','male'),(2,'naca','feMale'),(3,'nadcadca','unknown')]\n",
        "schema=['id','name','gender']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "#df1=df.sort(df.name.asc())\n",
        "\n",
        "df1=df.sort(df.name.desc())\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy8Ei5sIdHEN",
        "outputId": "f76604df-a480-4184-90d6-9f9816f8ce0a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+-------+\n",
            "| id|    name| gender|\n",
            "+---+--------+-------+\n",
            "|  1|  nayeer|   male|\n",
            "|  3|nadcadca|unknown|\n",
            "|  2|    naca| feMale|\n",
            "+---+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cast()\n",
        "from pyspark.sql.functions import when\n",
        "data=[(1,'nayeer','male'),(2,'naca','feMale'),(3,'nadcadca','unknown')]\n",
        "schema=['id','name','gender']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df1=df.select(df.name.cast('int'))\n",
        "df1.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NszoMiJqdc1A",
        "outputId": "8484d64d-77f9-483e-bcaa-91172746295a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #like()\n",
        "from pyspark.sql.functions import when\n",
        "data=[(1,'mayeer','male'),(2,'naca','feMale'),(3,'nadcadca','unknown')]\n",
        "schema=['id','name','gender']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df2=df.filter(df.name.like('m%')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqJQH5PIeiR2",
        "outputId": "2a491c1a-b916-49a0-992f-131ddb1eabd5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+\n",
            "| id|  name|gender|\n",
            "+---+------+------+\n",
            "|  1|mayeer|  male|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#where  functions\n",
        " #like()\n",
        "from pyspark.sql.functions import when\n",
        "data=[(1,'mayeer','male'),(2,'naca','feMale'),(3,'nadcadca','unknown')]\n",
        "schema=['id','name','gender']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.where(df.gender=='male').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXuZXtVL4UGV",
        "outputId": "38e319d4-862a-4f68-d87b-b43a24ab9d3b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+\n",
            "| id|  name|gender|\n",
            "+---+------+------+\n",
            "|  1|mayeer|  male|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Filter() functions\n",
        "\n",
        "from pyspark.sql.functions import when\n",
        "data=[(1,'mayeer','male'),(2,'naca','feMale'),(3,'nadcadca','unknown')]\n",
        "schema=['id','name','gender']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.filter(df.gender=='male').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMdYTEiu4sow",
        "outputId": "2b0f4c1d-cdf1-423e-b86b-315e6b9bf29b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+\n",
            "| id|  name|gender|\n",
            "+---+------+------+\n",
            "|  1|mayeer|  male|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\\22. distinct() & dropDuplicates() in PySpark\n",
        "from pyspark.sql.functions import when\n",
        "data=[(1,'mayeer','male'),(2,'naca','feMale'),(3,'nadcadca','unknown'),(4,'nadcadca','unknown'),(3,'nadcadca','unknown')]\n",
        "schema=['id','name','gender']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.distinct().show()\n",
        "df.dropDuplicates().show()\n",
        "df.dropDuplicates(['gender']).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ7vaX155ZVq",
        "outputId": "ae3a1347-be71-47da-f252-2409fa4d84a5"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+-------+\n",
            "| id|    name| gender|\n",
            "+---+--------+-------+\n",
            "|  1|  mayeer|   male|\n",
            "|  2|    naca| feMale|\n",
            "|  3|nadcadca|unknown|\n",
            "|  4|nadcadca|unknown|\n",
            "+---+--------+-------+\n",
            "\n",
            "+---+--------+-------+\n",
            "| id|    name| gender|\n",
            "+---+--------+-------+\n",
            "|  1|  mayeer|   male|\n",
            "|  2|    naca| feMale|\n",
            "|  3|nadcadca|unknown|\n",
            "|  4|nadcadca|unknown|\n",
            "+---+--------+-------+\n",
            "\n",
            "+---+--------+-------+\n",
            "| id|    name| gender|\n",
            "+---+--------+-------+\n",
            "|  2|    naca| feMale|\n",
            "|  1|  mayeer|   male|\n",
            "|  3|nadcadca|unknown|\n",
            "+---+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. orderBy() & sort() in PySpark\n",
        "from pyspark.sql.functions import when\n",
        "data=[(1013100,50000),(223231,50000),(379236973,989)]\n",
        "schema=['id','name',]\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.sort('name','id').show()\n",
        "df.orderBy(df.name.desc(),df.id.asc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jmRss0T7D6s",
        "outputId": "c25f21f0-6545-4142-dd56-2403dacd2eee"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|       id| name|\n",
            "+---------+-----+\n",
            "|379236973|  989|\n",
            "|   223231|50000|\n",
            "|  1013100|50000|\n",
            "+---------+-----+\n",
            "\n",
            "+---------+-----+\n",
            "|       id| name|\n",
            "+---------+-----+\n",
            "|   223231|50000|\n",
            "|  1013100|50000|\n",
            "|379236973|  989|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. orderBy() & sort() in PySpark\n",
        "from pyspark.sql.functions import when\n",
        "data=[(1013100,50000),(223231,50000),(379236973,989)]\n",
        "schema=['id','name',]\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.orderBy('name','id').show()\n",
        "df.sort(df.name.desc(),df.id.asc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnmxApRB9WMO",
        "outputId": "90599320-2181-47d0-9690-8eb867be86e5"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|       id| name|\n",
            "+---------+-----+\n",
            "|379236973|  989|\n",
            "|   223231|50000|\n",
            "|  1013100|50000|\n",
            "+---------+-----+\n",
            "\n",
            "+---------+-----+\n",
            "|       id| name|\n",
            "+---------+-----+\n",
            "|   223231|50000|\n",
            "|  1013100|50000|\n",
            "|379236973|  989|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. union() & unionAll() in PySpark\n",
        "from pyspark.sql.functions import when\n",
        "data1=[(10100,500),(2231,500),(3726973,98)]\n",
        "schema=['id','name',]\n",
        "data2=[(1013100,50000),(223231,50000),(379236973,989),(3726973,98)]\n",
        "schema=['id','name',]\n",
        "\n",
        "df1=spark.createDataFrame(data1,schema)\n",
        "df2=spark.createDataFrame(data2,schema)\n",
        "\n",
        "df1.show()\n",
        "df2.show()\n",
        "df1.unionAll(df2).show()\n",
        "df1.union(df2).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktE1ErBq9j3E",
        "outputId": "bc10b606-7bd0-44dd-f5a7-6b0982debf38"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+\n",
            "|     id|name|\n",
            "+-------+----+\n",
            "|  10100| 500|\n",
            "|   2231| 500|\n",
            "|3726973|  98|\n",
            "+-------+----+\n",
            "\n",
            "+---------+-----+\n",
            "|       id| name|\n",
            "+---------+-----+\n",
            "|  1013100|50000|\n",
            "|   223231|50000|\n",
            "|379236973|  989|\n",
            "|  3726973|   98|\n",
            "+---------+-----+\n",
            "\n",
            "+---------+-----+\n",
            "|       id| name|\n",
            "+---------+-----+\n",
            "|    10100|  500|\n",
            "|     2231|  500|\n",
            "|  3726973|   98|\n",
            "|  1013100|50000|\n",
            "|   223231|50000|\n",
            "|379236973|  989|\n",
            "|  3726973|   98|\n",
            "+---------+-----+\n",
            "\n",
            "+---------+-----+\n",
            "|       id| name|\n",
            "+---------+-----+\n",
            "|    10100|  500|\n",
            "|     2231|  500|\n",
            "|  3726973|   98|\n",
            "|  1013100|50000|\n",
            "|   223231|50000|\n",
            "|379236973|  989|\n",
            "|  3726973|   98|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. groupBy() in PySpark\n",
        "data=[(1,'naheey','M',5000,'IT'),\\\n",
        "      (2,'baheey','F',50100,'IT'),\\\n",
        "      (3,'gaheey','M',52000,'RIT'),\\\n",
        "      (4,'taheey','F',53000,'GIT'),\\\n",
        "      (5,'raheey','M',25000,'NIT'),\\\n",
        "      (6,'eaheey','F',15000,'KIT'),\\\n",
        "      (7,'waheey','M',95000,'CIT')]\n",
        "\n",
        "schema=['id','name','gender','salary','dep']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.groupBy('dep').count().show()\n",
        "df.groupBy('dep','gender').count().show()\n",
        "df.groupBy('dep').min('salary').show()\n",
        "df.groupBy('dep').max('salary').show()\n",
        "df.groupBy('dep').avg('salary').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRm7NFC2-1hL",
        "outputId": "53c9f3fd-6c69-4c21-b955-598527e46976"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|dep|count|\n",
            "+---+-----+\n",
            "| IT|    2|\n",
            "|RIT|    1|\n",
            "|CIT|    1|\n",
            "|KIT|    1|\n",
            "|NIT|    1|\n",
            "|GIT|    1|\n",
            "+---+-----+\n",
            "\n",
            "+---+------+-----+\n",
            "|dep|gender|count|\n",
            "+---+------+-----+\n",
            "|RIT|     M|    1|\n",
            "| IT|     M|    1|\n",
            "| IT|     F|    1|\n",
            "|KIT|     F|    1|\n",
            "|NIT|     M|    1|\n",
            "|GIT|     F|    1|\n",
            "|CIT|     M|    1|\n",
            "+---+------+-----+\n",
            "\n",
            "+---+-----------+\n",
            "|dep|min(salary)|\n",
            "+---+-----------+\n",
            "| IT|       5000|\n",
            "|RIT|      52000|\n",
            "|CIT|      95000|\n",
            "|KIT|      15000|\n",
            "|NIT|      25000|\n",
            "|GIT|      53000|\n",
            "+---+-----------+\n",
            "\n",
            "+---+-----------+\n",
            "|dep|max(salary)|\n",
            "+---+-----------+\n",
            "| IT|      50100|\n",
            "|RIT|      52000|\n",
            "|CIT|      95000|\n",
            "|KIT|      15000|\n",
            "|NIT|      25000|\n",
            "|GIT|      53000|\n",
            "+---+-----------+\n",
            "\n",
            "+---+-----------+\n",
            "|dep|avg(salary)|\n",
            "+---+-----------+\n",
            "| IT|    27550.0|\n",
            "|RIT|    52000.0|\n",
            "|CIT|    95000.0|\n",
            "|KIT|    15000.0|\n",
            "|NIT|    25000.0|\n",
            "|GIT|    53000.0|\n",
            "+---+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#agg()\n",
        "#25. groupBy() in PySpark\n",
        "data=[(1,'naheey','M',5000,'IT'),\\\n",
        "      (2,'baheey','F',50100,'IT'),\\\n",
        "      (3,'gaheey','M',52000,'RIT'),\\\n",
        "      (4,'taheey','F',53000,'GIT'),\\\n",
        "      (5,'raheey','M',25000,'NIT'),\\\n",
        "      (6,'eaheey','F',15000,'KIT'),\\\n",
        "      (7,'waheey','M',95000,'CIT')]\n",
        "\n",
        "schema=['id','name','gender','salary','dep']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.groupBy('dep').count().show()\n",
        "\n",
        "from pyspark.sql.functions import count,min,max\n",
        "df.groupBy('dep').agg(count('*').alias('countOfEmps'),\\\n",
        "                      min('salary').alias('minSal'),\\\n",
        "                      max('salary').alias('maxSal')).show()"
      ],
      "metadata": {
        "id": "MAwJ1YHqBkmM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa0e47a7-e398-4b1a-ae99-81b5c47eb8c3"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|dep|count|\n",
            "+---+-----+\n",
            "| IT|    2|\n",
            "|RIT|    1|\n",
            "|CIT|    1|\n",
            "|KIT|    1|\n",
            "|NIT|    1|\n",
            "|GIT|    1|\n",
            "+---+-----+\n",
            "\n",
            "+---+-----------+------+------+\n",
            "|dep|countOfEmps|minSal|maxSal|\n",
            "+---+-----------+------+------+\n",
            "| IT|          2|  5000| 50100|\n",
            "|RIT|          1| 52000| 52000|\n",
            "|CIT|          1| 95000| 95000|\n",
            "|KIT|          1| 15000| 15000|\n",
            "|NIT|          1| 25000| 25000|\n",
            "|GIT|          1| 53000| 53000|\n",
            "+---+-----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#27. unionByName() function in PySpark\n",
        "from pyspark.sql.functions import when\n",
        "data1=[(10100,'king'),(2231,'prince'),(3726973,'ada')]\n",
        "schema1=['id','name',]\n",
        "data2=[(10100,'king'),(379236973,'adada'),(3726973,'dghi')]\n",
        "schema2=['id','gtae',]\n",
        "\n",
        "df1=spark.createDataFrame(data1,schema1)\n",
        "df2=spark.createDataFrame(data2,schema2)\n",
        "\n",
        "df1.union(df2).show()\n",
        "\n",
        "df1.unionByName(allowMissingColumns=True,other=df2).show()"
      ],
      "metadata": {
        "id": "GHkIOHtw8Th2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362382c3-b78b-41a8-db90-95c2aba4ec14"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+\n",
            "|       id|  name|\n",
            "+---------+------+\n",
            "|    10100|  king|\n",
            "|     2231|prince|\n",
            "|  3726973|   ada|\n",
            "|    10100|  king|\n",
            "|379236973| adada|\n",
            "|  3726973|  dghi|\n",
            "+---------+------+\n",
            "\n",
            "+---------+------+-----+\n",
            "|       id|  name| gtae|\n",
            "+---------+------+-----+\n",
            "|    10100|  king| NULL|\n",
            "|     2231|prince| NULL|\n",
            "|  3726973|   ada| NULL|\n",
            "|    10100|  NULL| king|\n",
            "|379236973|  NULL|adada|\n",
            "|  3726973|  NULL| dghi|\n",
            "+---------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#28. select() function in PySpark\n",
        "from pyspark.sql.functions import lit\n",
        "data=[('nayeer','male',20000),('naushad','male',300000)]\n",
        "schema=['name','gender','salary']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "#select single or multi columns\n",
        "df.select('salary','name').show()\n",
        "df.select(df.name,df.gender).show()\n",
        "df.select(df['name'],df['gender']).show()\n",
        "\n",
        "#using col() function\n",
        "from pyspark.sql.functions import col\n",
        "df.select(col('salary'),col('name')).show()\n",
        "df.select(['gender','name']).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MrkS9NQCQLv",
        "outputId": "65853843-a29f-40d9-a519-5cf0ffac3253"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+\n",
            "|salary|   name|\n",
            "+------+-------+\n",
            "| 20000| nayeer|\n",
            "|300000|naushad|\n",
            "+------+-------+\n",
            "\n",
            "+-------+------+\n",
            "|   name|gender|\n",
            "+-------+------+\n",
            "| nayeer|  male|\n",
            "|naushad|  male|\n",
            "+-------+------+\n",
            "\n",
            "+-------+------+\n",
            "|   name|gender|\n",
            "+-------+------+\n",
            "| nayeer|  male|\n",
            "|naushad|  male|\n",
            "+-------+------+\n",
            "\n",
            "+------+-------+\n",
            "|salary|   name|\n",
            "+------+-------+\n",
            "| 20000| nayeer|\n",
            "|300000|naushad|\n",
            "+------+-------+\n",
            "\n",
            "+------+-------+\n",
            "|gender|   name|\n",
            "+------+-------+\n",
            "|  male| nayeer|\n",
            "|  male|naushad|\n",
            "+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#select() for all column\n",
        "df.select([col for col in df.columns]).show()\n",
        "df.select('*').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe_svXymD5pD",
        "outputId": "b0d72a7d-1988-40b5-f75b-f5bd6ae2f893"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------+\n",
            "|   name|gender|salary|\n",
            "+-------+------+------+\n",
            "| nayeer|  male| 20000|\n",
            "|naushad|  male|300000|\n",
            "+-------+------+------+\n",
            "\n",
            "+-------+------+------+\n",
            "|   name|gender|salary|\n",
            "+-------+------+------+\n",
            "| nayeer|  male| 20000|\n",
            "|naushad|  male|300000|\n",
            "+-------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#29. join() function in PySpark\n",
        "from pyspark.sql.functions import when\n",
        "data1=[(1,'nwafa',2000,2),(2,'fsfs',5555,1),(3,'abcd',1111,1)]\n",
        "schema1=['id','name','salary','dep']\n",
        "data2=[(1,'nwafa'),(379236973,'adada'),(3726973,'dghi')]\n",
        "schema2=['id','gtae',]\n",
        "\n",
        "empDf=spark.createDataFrame(data1,schema1)\n",
        "depDf=spark.createDataFrame(data2,schema2)\n",
        "empDf.show()\n",
        "depDf.show()\n",
        "\n",
        "empDf.join(depDf,empDf.dep==depDf.id,'inner').show()\n",
        "empDf.join(depDf,empDf.dep==depDf.id,'left').show()\n",
        "empDf.join(depDf,empDf.dep==depDf.id,'right').show()\n",
        "empDf.join(depDf,empDf.dep==depDf.id,'full').show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md2yBm6yE3gr",
        "outputId": "97cc0682-92ad-4c43-ca22-7e72bcc66c6b"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+---+\n",
            "| id| name|salary|dep|\n",
            "+---+-----+------+---+\n",
            "|  1|nwafa|  2000|  2|\n",
            "|  2| fsfs|  5555|  1|\n",
            "|  3| abcd|  1111|  1|\n",
            "+---+-----+------+---+\n",
            "\n",
            "+---------+-----+\n",
            "|       id| gtae|\n",
            "+---------+-----+\n",
            "|        1|nwafa|\n",
            "|379236973|adada|\n",
            "|  3726973| dghi|\n",
            "+---------+-----+\n",
            "\n",
            "+---+----+------+---+---+-----+\n",
            "| id|name|salary|dep| id| gtae|\n",
            "+---+----+------+---+---+-----+\n",
            "|  2|fsfs|  5555|  1|  1|nwafa|\n",
            "|  3|abcd|  1111|  1|  1|nwafa|\n",
            "+---+----+------+---+---+-----+\n",
            "\n",
            "+---+-----+------+---+----+-----+\n",
            "| id| name|salary|dep|  id| gtae|\n",
            "+---+-----+------+---+----+-----+\n",
            "|  1|nwafa|  2000|  2|NULL| NULL|\n",
            "|  2| fsfs|  5555|  1|   1|nwafa|\n",
            "|  3| abcd|  1111|  1|   1|nwafa|\n",
            "+---+-----+------+---+----+-----+\n",
            "\n",
            "+----+----+------+----+---------+-----+\n",
            "|  id|name|salary| dep|       id| gtae|\n",
            "+----+----+------+----+---------+-----+\n",
            "|   3|abcd|  1111|   1|        1|nwafa|\n",
            "|   2|fsfs|  5555|   1|        1|nwafa|\n",
            "|NULL|NULL|  NULL|NULL|379236973|adada|\n",
            "|NULL|NULL|  NULL|NULL|  3726973| dghi|\n",
            "+----+----+------+----+---------+-----+\n",
            "\n",
            "+----+-----+------+----+---------+-----+\n",
            "|  id| name|salary| dep|       id| gtae|\n",
            "+----+-----+------+----+---------+-----+\n",
            "|   2| fsfs|  5555|   1|        1|nwafa|\n",
            "|   3| abcd|  1111|   1|        1|nwafa|\n",
            "|   1|nwafa|  2000|   2|     NULL| NULL|\n",
            "|NULL| NULL|  NULL|NULL|  3726973| dghi|\n",
            "|NULL| NULL|  NULL|NULL|379236973|adada|\n",
            "+----+-----+------+----+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D5CvfYKqHYO6"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30. join() function in PySpark Continuation\n",
        "from pyspark.sql.functions import when\n",
        "data1=[(1,'nwafa',2000,2),(2,'fsfs',5555,1),(3,'abcd',1111,1)]\n",
        "schema1=['id','name','salary','dep']\n",
        "data2=[(1,'nwafa'),(379236973,'adada'),(3726973,'dghi')]\n",
        "schema2=['id','gtae',]\n",
        "\n",
        "empDf=spark.createDataFrame(data1,schema1)\n",
        "depDf=spark.createDataFrame(data2,schema2)\n",
        "# empDf.show()\n",
        "# depDf.show()\n",
        "\n",
        "empDf.join(depDf,empDf.dep==depDf.id,'leftsemi').show()\n",
        "empDf.join(depDf,empDf.dep==depDf.id,'leftanti').show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3VhwU7bJMuW",
        "outputId": "eda9edd9-28f7-4231-e934-1febf222cc23"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+------+---+\n",
            "| id|name|salary|dep|\n",
            "+---+----+------+---+\n",
            "|  2|fsfs|  5555|  1|\n",
            "|  3|abcd|  1111|  1|\n",
            "+---+----+------+---+\n",
            "\n",
            "+---+-----+------+---+\n",
            "| id| name|salary|dep|\n",
            "+---+-----+------+---+\n",
            "|  1|nwafa|  2000|  2|\n",
            "+---+-----+------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1=[(1,'nwafa',0),(2,'fsfs',1),(3,'abcd',2)]\n",
        "schema1=['id','name','manager_id']\n",
        "df=spark.createDataFrame(data1,schema1)\n",
        "from pyspark.sql.functions import col\n",
        "df.alias('emp').join(df.alias('manager'),col('emp.manager_id')==col('manager.id'),'left')\\\n",
        "    .select(col('emp.name').alias('empName'),col('manager.name').alias('managerName')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OelDG74GLQhZ",
        "outputId": "902b77e5-e251-408b-9086-e6f339cfbcf0"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|empName|managerName|\n",
            "+-------+-----------+\n",
            "|  nwafa|       NULL|\n",
            "|   fsfs|      nwafa|\n",
            "|   abcd|       fsfs|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#31. pivot() function in PySpark\n",
        "data=[(1,'naheey','M',5000,'IT'),\\\n",
        "      (2,'baheey','F',50100,'IT'),\\\n",
        "      (3,'gaheey','M',52000,'RIT'),\\\n",
        "      (4,'taheey','F',53000,'GIT'),\\\n",
        "      (5,'raheey','M',25000,'NIT'),\\\n",
        "      (6,'eaheey','F',15000,'KIT'),\\\n",
        "      (7,'waheey','M',95000,'CIT')]\n",
        "\n",
        "schema=['id','name','gender','salary','dep']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "\n",
        "df.groupBy('dep').pivot('gender').count().show()"
      ],
      "metadata": {
        "id": "Rs-er6r-NFqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9455c95e-c522-4419-f399-c6eebd5842ae"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+------+---+\n",
            "| id|  name|gender|salary|dep|\n",
            "+---+------+------+------+---+\n",
            "|  1|naheey|     M|  5000| IT|\n",
            "|  2|baheey|     F| 50100| IT|\n",
            "|  3|gaheey|     M| 52000|RIT|\n",
            "|  4|taheey|     F| 53000|GIT|\n",
            "|  5|raheey|     M| 25000|NIT|\n",
            "|  6|eaheey|     F| 15000|KIT|\n",
            "|  7|waheey|     M| 95000|CIT|\n",
            "+---+------+------+------+---+\n",
            "\n",
            "+---+----+----+\n",
            "|dep|   F|   M|\n",
            "+---+----+----+\n",
            "|CIT|NULL|   1|\n",
            "|KIT|   1|NULL|\n",
            "|NIT|NULL|   1|\n",
            "| IT|   1|   1|\n",
            "|RIT|NULL|   1|\n",
            "|GIT|   1|NULL|\n",
            "+---+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#32. unpivot Dataframe in PySpark\n",
        "data=[('IT',8,5),\\\n",
        "      ('Payroll',3,2),\\\n",
        "      ('HR',2,4)\\\n",
        "      ]\n",
        "\n",
        "schema=['dep','male','female']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "\n",
        "from pyspark.sql.functions import expr\n",
        "df.select('dep',expr(\"stack(2,'male',male,'female',female) as (gender, count)\")).show()\n",
        "# df.select('dep',expr(\"stack(2,'tama',male,'tonga',female) as (gender, count)\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeHt8OgIovvo",
        "outputId": "12f49f9e-f2b7-487f-a812-676ee383736a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+------+\n",
            "|    dep|male|female|\n",
            "+-------+----+------+\n",
            "|     IT|   8|     5|\n",
            "|Payroll|   3|     2|\n",
            "|     HR|   2|     4|\n",
            "+-------+----+------+\n",
            "\n",
            "+-------+------+-----+\n",
            "|    dep|gender|count|\n",
            "+-------+------+-----+\n",
            "|     IT|  male|    8|\n",
            "|     IT|female|    5|\n",
            "|Payroll|  male|    3|\n",
            "|Payroll|female|    2|\n",
            "|     HR|  male|    2|\n",
            "|     HR|female|    4|\n",
            "+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#33. fill() & fillna() functions in PySpark\n",
        "from pyspark.sql.functions import lit\n",
        "data=[('nayeer','male',None),('naushad',None,300000)]\n",
        "schema=['name','gender','salary']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "\n",
        "df.na.fill('unknown',['gender']).show()\n",
        "\n",
        "df.fillna('unknown',['salary']).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tlEF7ndpyYY",
        "outputId": "7b7ef891-f786-4da8-fdc3-791599ffb874"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------+\n",
            "|   name|gender|salary|\n",
            "+-------+------+------+\n",
            "| nayeer|  male|  NULL|\n",
            "|naushad|  NULL|300000|\n",
            "+-------+------+------+\n",
            "\n",
            "+-------+-------+------+\n",
            "|   name| gender|salary|\n",
            "+-------+-------+------+\n",
            "| nayeer|   male|  NULL|\n",
            "|naushad|unknown|300000|\n",
            "+-------+-------+------+\n",
            "\n",
            "+-------+------+------+\n",
            "|   name|gender|salary|\n",
            "+-------+------+------+\n",
            "| nayeer|  male|  NULL|\n",
            "|naushad|  NULL|300000|\n",
            "+-------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#34. sample() function in PySpark\n",
        "df=spark.range(start=1, end=101)\n",
        "df1=df.sample(fraction=0.1,seed=123)\n",
        "df2=df.sample(fraction=0.1,seed=123)  # we put seed tjust to get same random number\n",
        "df1.show()\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytpa7lg8uR-A",
        "outputId": "a55fccb9-41c7-462b-c583-def1e864eb29"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "| 37|\n",
            "| 38|\n",
            "| 42|\n",
            "| 44|\n",
            "| 70|\n",
            "| 76|\n",
            "| 85|\n",
            "| 98|\n",
            "+---+\n",
            "\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "| 37|\n",
            "| 38|\n",
            "| 42|\n",
            "| 44|\n",
            "| 70|\n",
            "| 76|\n",
            "| 85|\n",
            "| 98|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#35. collect() function in PySpark\n",
        "from pyspark.sql.functions import lit\n",
        "data=[('nayeer','male',None),('naushad',None,300000)]\n",
        "schema=['name','gender','salary']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "\n",
        "dataRows=df.collect() #it is similar to select() but here the op is collected in array\n",
        "print(dataRows)\n",
        "print(dataRows[0])\n",
        "# print(dataRows[1])\n",
        "print(dataRows[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjfqFkDfwCII",
        "outputId": "d8252bf5-be74-4a0f-c04f-53f7305ae8b8"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------+\n",
            "|   name|gender|salary|\n",
            "+-------+------+------+\n",
            "| nayeer|  male|  NULL|\n",
            "|naushad|  NULL|300000|\n",
            "+-------+------+------+\n",
            "\n",
            "[Row(name='nayeer', gender='male', salary=None), Row(name='naushad', gender=None, salary=300000)]\n",
            "Row(name='nayeer', gender='male', salary=None)\n",
            "nayeer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#36. DataFrame.transform() function in PySpark\n",
        "\n",
        "data=[('nayeer','male',5678),('naushad',None,300000)]\n",
        "schema=['name','gender','salary']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "\n",
        "from pyspark.sql.functions import upper\n",
        "\n",
        "def convertToUpper(df):\n",
        "  return df.withColumn('name',upper(df.name))\n",
        "\n",
        "def doubleTheSalary(df):\n",
        "  return df.withColumn('salary',df.salary*2)\n",
        "\n",
        "df1=df.transform(convertToUpper)\\\n",
        "      .transform(doubleTheSalary)\n",
        "df1.show()"
      ],
      "metadata": {
        "id": "WGUkuWxfxfAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b85848e4-1517-4e00-90c2-5c1d2ae83af0"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+------+\n",
            "|   name|gender|salary|\n",
            "+-------+------+------+\n",
            "| nayeer|  male|  5678|\n",
            "|naushad|  NULL|300000|\n",
            "+-------+------+------+\n",
            "\n",
            "+-------+------+------+\n",
            "|   name|gender|salary|\n",
            "+-------+------+------+\n",
            "| NAYEER|  male| 11356|\n",
            "|NAUSHAD|  NULL|600000|\n",
            "+-------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#37. pyspark.sql.functions.transform()\n",
        "data=[(1,'nayeer',['azure','dot']),(2,'naaefef',['aafae','tod'])]\n",
        "schema=['id','name','skills']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "from pyspark.sql.functions import transform\n",
        "df.select('id','name',transform('skills',lambda x: upper(x)).alias('skills')).show()\n",
        "\n",
        "def convertToUpper(x):\n",
        "  return upper(x)\n",
        "\n",
        "from pyspark.sql.functions import transform, upper\n",
        "df.select(transform('skills',convertToUpper).alias('skills')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc3EJVaWJXJE",
        "outputId": "f8cd0a31-b4b4-49cd-f3df-f06846d4e73b"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------------+\n",
            "| id|   name|      skills|\n",
            "+---+-------+------------+\n",
            "|  1| nayeer|[azure, dot]|\n",
            "|  2|naaefef|[aafae, tod]|\n",
            "+---+-------+------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+---+-------+------------+\n",
            "| id|   name|      skills|\n",
            "+---+-------+------------+\n",
            "|  1| nayeer|[AZURE, DOT]|\n",
            "|  2|naaefef|[AAFAE, TOD]|\n",
            "+---+-------+------------+\n",
            "\n",
            "+------------+\n",
            "|      skills|\n",
            "+------------+\n",
            "|[AZURE, DOT]|\n",
            "|[AAFAE, TOD]|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#38. createOrReplaceTempView() function in PySpark\n",
        "data=[(1,'nayeer','male',5678),(2,'naushad',None,300000)]\n",
        "schema=['id','name','gender','salary']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "df.createOrReplaceTempView('employees')\n",
        "df1=spark.sql('SELECT * FROM employees')\n",
        "df1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwW7Nsa1NHFB",
        "outputId": "8111f986-2002-404e-ea20-4aebb0455dba"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+------+\n",
            "| id|   name|gender|salary|\n",
            "+---+-------+------+------+\n",
            "|  1| nayeer|  male|  5678|\n",
            "|  2|naushad|  NULL|300000|\n",
            "+---+-------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#40. UDF(user defined function)\n",
        "data =[(1,'nayeer',2000,500),(2,'naushad',4000,1000)]\n",
        "\n",
        "schema=['id','name','salary','bonus']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjRrnRmHRlk8",
        "outputId": "7aedcf67-5d59-48ed-80dd-5bea85635fce"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+-----+\n",
            "| id|   name|salary|bonus|\n",
            "+---+-------+------+-----+\n",
            "|  1| nayeer|  2000|  500|\n",
            "|  2|naushad|  4000| 1000|\n",
            "+---+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def totalPay(s,b):\n",
        "  return s+b\n",
        "\n",
        "from pyspark.sql.functions import udf,col\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "TotalPay=udf(lambda x,y: totalPay(x,y),IntegerType())\n",
        "\n",
        "df.select('*',TotalPay(col('salary'),col('bonus')).alias('totalPay')).show()\n",
        "df.withColumn('tolPay',TotalPay(df.salary,df.bonus)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ab6N-7BUW3A",
        "outputId": "c566c76a-8315-4b80-9487-770cadb28b86"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+-----+--------+\n",
            "| id|   name|salary|bonus|totalPay|\n",
            "+---+-------+------+-----+--------+\n",
            "|  1| nayeer|  2000|  500|    2500|\n",
            "|  2|naushad|  4000| 1000|    5000|\n",
            "+---+-------+------+-----+--------+\n",
            "\n",
            "+---+-------+------+-----+------+\n",
            "| id|   name|salary|bonus|tolPay|\n",
            "+---+-------+------+-----+------+\n",
            "|  1| nayeer|  2000|  500|  2500|\n",
            "|  2|naushad|  4000| 1000|  5000|\n",
            "+---+-------+------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#41. Convert RDD to Dataframe in PySpark\n",
        "data=[(1,'nayeer'),(2,'wasim')]\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "print(rdd.collect())"
      ],
      "metadata": {
        "id": "TEeNTf90WpY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d68cadf5-a698-49ef-8944-24aadba0ab73"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 'nayeer'), (2, 'wasim')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=rdd.toDF(['id','name'])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4_dyRqMH3IH",
        "outputId": "04825058-0f40-44f8-dd3e-f0296939b8a1"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|nayeer|\n",
            "|  2| wasim|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1=spark.createDataFrame(rdd,['id','name'])\n",
        "df1.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAdieUeMISRf",
        "outputId": "07b360aa-35a7-455b-d432-bf0d274765a8"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id|  name|\n",
            "+---+------+\n",
            "|  1|nayeer|\n",
            "|  2| wasim|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#42. map() transformation in PySpark\n",
        "data=[('king','nayeer'),('kohli','wasim')]\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "rdd1=rdd.map(lambda x: x + (x[0]+x[1],) )\n",
        "print(rdd1.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMho0F7HIalG",
        "outputId": "d45fc56e-ed66-4521-e888-6d564bf807be"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('king', 'nayeer', 'kingnayeer'), ('kohli', 'wasim', 'kohliwasim')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def FullName(x):\n",
        "  x=x+(x[0]+' '+x[1],)\n",
        "  return x\n",
        "\n",
        "data=[('king','nayeer'),('kohli','wasim')]\n",
        "rdd1=spark.createDataFrame(data,['fn','ln'])\n",
        "rdd1=rdd.map(lambda x: FullName(x))\n",
        "df1=rdd1.toDF(['fn','ln','FullName'])\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6L7kxHyJp4J",
        "outputId": "ea9aec70-dbb8-4997-834b-1d1d0bc6c949"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+\n",
            "|   fn|    ln|   FullName|\n",
            "+-----+------+-----------+\n",
            "| king|nayeer|king nayeer|\n",
            "|kohli| wasim|kohli wasim|\n",
            "+-----+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[('king','nayeer'),('kohli','wasim')]\n",
        "rdd1=spark.createDataFrame(data,['fn','ln'])\n",
        "rdd1=rdd.map(lambda x: x + (x[0]+' '+x[1],) )\n",
        "df2=rdd1.toDF(['fn','ln','FullName'])\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZUnxnG_LMK0",
        "outputId": "e0249c7f-e4e1-4ac3-d2d4-592f317c9a0e"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+\n",
            "|   fn|    ln|   FullName|\n",
            "+-----+------+-----------+\n",
            "| king|nayeer|king nayeer|\n",
            "|kohli| wasim|kohli wasim|\n",
            "+-----+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#43. flatMap() transformation in PySpark\n",
        "data=[('king nayeer'),('kohli wasim')]\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "\n",
        "for item in rdd.collect():\n",
        "  print(item)\n",
        "\n",
        "\n",
        "rdd1=rdd.flatMap(lambda x: x.split(' '))\n",
        "for item in rdd1.collect():\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGK1fROgLO8r",
        "outputId": "1785b6a0-19d9-430d-ff8b-7ae193d7da13"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "king nayeer\n",
            "kohli wasim\n",
            "king\n",
            "nayeer\n",
            "kohli\n",
            "wasim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#44. partitionBy function in PySpark\n",
        "data=[(1,'nayeer','male','IT'),(2,'yayeer','female','CIT'),(7,'nareer','male','KIIT')]\n",
        "schema=['id','name','gender','dep']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.write.parquet(path='/content/parquetop12',mode='overwrite',partitionBy='gender')\n",
        "df.write.parquet(path='/content/parquetop11',mode='overwrite',partitionBy=['dep','gender'])"
      ],
      "metadata": {
        "id": "TKjuTp9xMqsM"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#45. from_json() function to convert json string in to MapType in Pyspark\n",
        "data=[('nayeer','{\"hair\":\"black\",\"eye\":\"brown\"}')]\n",
        "schema=['name','props']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show(truncate=False)\n",
        "df.printSchema()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4fUniQKPY3w",
        "outputId": "6c0770b7-55c6-4986-bb7c-1a5418836e7d"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------------------+\n",
            "|name  |props                         |\n",
            "+------+------------------------------+\n",
            "|nayeer|{\"hair\":\"black\",\"eye\":\"brown\"}|\n",
            "+------+------------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- props: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import from_json\n",
        "from pyspark.sql.types import MapType, StringType\n",
        "MapTypeSchema = MapType(StringType(), StringType())\n",
        "#propsMap column with MapType gets generates fropm json string\n",
        "df1= df.withColumn('propsMap',from_json(df.props,MapTypeSchema))\n",
        "df1.show(truncate=False)\n",
        "df1.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48K8P8oBRfoB",
        "outputId": "23bc9629-9cb5-4a3e-d70f-e611d0208d3e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------------------+-----------------------------+\n",
            "|name  |props                         |propsMap                     |\n",
            "+------+------------------------------+-----------------------------+\n",
            "|nayeer|{\"hair\":\"black\",\"eye\":\"brown\"}|{hair -> black, eye -> brown}|\n",
            "+------+------------------------------+-----------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- props: string (nullable = true)\n",
            " |-- propsMap: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#accessing 'eye' key from MapType column 'propsMap'\n",
        "df2=df1.withColumn('eye',df1.propsMap.eye)\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUiR6auGRWLL",
        "outputId": "ff4d7c87-a5a1-478c-e221-6e4dde6be296"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------------------+-----------------------------+-----+\n",
            "|name  |props                         |propsMap                     |eye  |\n",
            "+------+------------------------------+-----------------------------+-----+\n",
            "|nayeer|{\"hair\":\"black\",\"eye\":\"brown\"}|{hair -> black, eye -> brown}|brown|\n",
            "+------+------------------------------+-----------------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df2=df1.withColumn('eye',df1.propsMap.eye)\\\n",
        ".withColumn('hair',df1.propsMap.hair)\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJmQBeSuRb_U",
        "outputId": "55bdd66b-17f3-4f30-d0bc-809e3ef71961"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------------------+-----------------------------+-----+-----+\n",
            "|name  |props                         |propsMap                     |eye  |hair |\n",
            "+------+------------------------------+-----------------------------+-----+-----+\n",
            "|nayeer|{\"hair\":\"black\",\"eye\":\"brown\"}|{hair -> black, eye -> brown}|brown|black|\n",
            "+------+------------------------------+-----------------------------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#46. from_json() function to convert json string into StructType in Pyspark\n",
        "data=[('nayeer','{\"hair\":\"black\",\"eye\":\"brown\"}')]\n",
        "schema=['name','props']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "# df.show(truncate=False)\n",
        "# df.printSchema()\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import from_json\n",
        "from pyspark.sql.types import MapType, StringType\n",
        "structSchema = StructType([\\\n",
        "                           StructField('hair',StringType()),\\\n",
        "                           StructField('eye',StringType()),\\\n",
        "                           ])\n",
        "\n",
        "MapTypeSchema = MapType(StringType(), StringType())\n",
        "#propsMap column with MapType gets generates fropm json string\n",
        "df1= df.withColumn('propsStruct',from_json(df.props,structSchema))\n",
        "df1.show(truncate=False)\n",
        "df1.printSchema()\n",
        "\n",
        "#accessing 'eye' key from MapType column 'propsMap'\n",
        "df2=df1.withColumn('eye',df1.propsStruct.eye)\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzayZOgVUOXm",
        "outputId": "78ef8e75-1762-468f-9571-35298bc170cf"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------------------------+--------------+\n",
            "|name  |props                         |propsStruct   |\n",
            "+------+------------------------------+--------------+\n",
            "|nayeer|{\"hair\":\"black\",\"eye\":\"brown\"}|{black, brown}|\n",
            "+------+------------------------------+--------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- props: string (nullable = true)\n",
            " |-- propsStruct: struct (nullable = true)\n",
            " |    |-- hair: string (nullable = true)\n",
            " |    |-- eye: string (nullable = true)\n",
            "\n",
            "+------+------------------------------+--------------+-----+\n",
            "|name  |props                         |propsStruct   |eye  |\n",
            "+------+------------------------------+--------------+-----+\n",
            "|nayeer|{\"hair\":\"black\",\"eye\":\"brown\"}|{black, brown}|brown|\n",
            "+------+------------------------------+--------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#47. to_json() function in PySpark\n",
        "from pyspark.sql.functions import to_json\n",
        "from pyspark.sql.types import StructType, StructField , StringType\n",
        "\n",
        "data=[('nayeer',{\"hair\":\"black\",\"eye\":\"brown\"})]\n",
        "schema=['name','properties']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "#here 'prop' column will get generated as json string\n",
        "df1=df.withColumn('props',to_json(df.properties))\n",
        "df1.show(truncate=False)\n",
        "df1.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44kt5K7AYzhc",
        "outputId": "b4a2eece-b26e-4e52-e92a-048989a6bf4f"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+\n",
            "|  name|          properties|\n",
            "+------+--------------------+\n",
            "|nayeer|{eye -> brown, ha...|\n",
            "+------+--------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+------+-----------------------------+------------------------------+\n",
            "|name  |properties                   |props                         |\n",
            "+------+-----------------------------+------------------------------+\n",
            "|nayeer|{eye -> brown, hair -> black}|{\"eye\":\"brown\",\"hair\":\"black\"}|\n",
            "+------+-----------------------------+------------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- props: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField , StringType , StructType\n",
        "data=[('maheer',('black','brown'))]\n",
        "schema=StructType([\\\n",
        "                   StructField('name',StringType()),\\\n",
        "                   StructField('properties',StructType([StructField('hair',StringType()),StructField('eye',StringType())]))\\\n",
        "                   ])\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show()\n",
        "df.printSchema()\n",
        "\n",
        "from pyspark.sql.types import json\n",
        "#here 'prop' column will get generated as json string\n",
        "df1=df.withColumn('props',to_json(df.properties))\n",
        "df1.show(truncate=False)\n",
        "df1.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV00dQ4Pa7Lm",
        "outputId": "488f7590-6ff7-4c86-b928-706413379a40"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+\n",
            "|  name|    properties|\n",
            "+------+--------------+\n",
            "|maheer|{black, brown}|\n",
            "+------+--------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: struct (nullable = true)\n",
            " |    |-- hair: string (nullable = true)\n",
            " |    |-- eye: string (nullable = true)\n",
            "\n",
            "+------+--------------+------------------------------+\n",
            "|name  |properties    |props                         |\n",
            "+------+--------------+------------------------------+\n",
            "|maheer|{black, brown}|{\"hair\":\"black\",\"eye\":\"brown\"}|\n",
            "+------+--------------+------------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: struct (nullable = true)\n",
            " |    |-- hair: string (nullable = true)\n",
            " |    |-- eye: string (nullable = true)\n",
            " |-- props: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#48. json_tuple() function in PySpark\n",
        "data=[('nayeer','{\"hair\":\"black\",\"eye\":\"brown\" , \"skin\":\"brown\"}'),\n",
        "      ('golden','{\"hair\":\"golden\",\"eye\":\"blue\" , \"skin\":\"white\"}')]\n",
        "schema=['name','props']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show(truncate = False)\n",
        "df.printSchema()\n",
        "\n",
        "from pyspark.sql.functions import json_tuple\n",
        "df2=df.select(df.name, json_tuple(df.props,'hair','skin').alias('hair','skin'))\n",
        "df2.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7zdFoVcdA05",
        "outputId": "913b60b1-dbe0-4ce6-f19e-fccf2831c7b6"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------------------------------------+\n",
            "|name  |props                                          |\n",
            "+------+-----------------------------------------------+\n",
            "|nayeer|{\"hair\":\"black\",\"eye\":\"brown\" , \"skin\":\"brown\"}|\n",
            "|golden|{\"hair\":\"golden\",\"eye\":\"blue\" , \"skin\":\"white\"}|\n",
            "+------+-----------------------------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- props: string (nullable = true)\n",
            "\n",
            "+------+------+-----+\n",
            "|name  |hair  |skin |\n",
            "+------+------+-----+\n",
            "|nayeer|black |brown|\n",
            "|golden|golden|white|\n",
            "+------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#49. get_ json_object() function in PySpark\n",
        "data=[('nayeer','{\"address\":{\"city\":\"hyd\",\"state\":\"telegana\"},\"gender\":\"male\"}'),\n",
        "      ('kingi','{\"address\":{\"city\":\"dyh\",\"state\":\"gana\"},\"gender\":\"female\"}')]\n",
        "\n",
        "schema=['name','props']\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.show(truncate=False)\n",
        "df.printSchema()\n",
        "from pyspark.sql.functions import get_json_object\n",
        "df1=df.select('name',get_json_object('props','$.address.city').alias('city'))\n",
        "df1.show(truncate=False)\n",
        "df1.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mln0sMX9hgRn",
        "outputId": "520a0e5f-f4ce-4ba8-f0a1-283b22ca200d"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------------------------------------------------------+\n",
            "|name  |props                                                        |\n",
            "+------+-------------------------------------------------------------+\n",
            "|nayeer|{\"address\":{\"city\":\"hyd\",\"state\":\"telegana\"},\"gender\":\"male\"}|\n",
            "|kingi |{\"address\":{\"city\":\"dyh\",\"state\":\"gana\"},\"gender\":\"female\"}  |\n",
            "+------+-------------------------------------------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- props: string (nullable = true)\n",
            "\n",
            "+------+----+\n",
            "|name  |city|\n",
            "+------+----+\n",
            "|nayeer|hyd |\n",
            "|kingi |dyh |\n",
            "+------+----+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#50. Date functions in PySpark | current_date(), to_date(), date_format() functions\n",
        "from pyspark.sql.functions import current_date,date_format,lit,to_date\n",
        "\n",
        "df=spark.range(2)\n",
        "\n",
        "#datatype default format is yyyy-MM-dd\n",
        "#gives current date in yyyy-MM-dd format\n",
        "df.withColumn('todaysDate',current_date()).show()\n",
        "\n",
        "#converts yyyy-MM-dd datatypes to specified format\n",
        "df.withColumn('newFormat',date_format(lit('2024-05-14'),'MM-dd-yyyy')).show()\n",
        "\n",
        "#converts string values of date to DateType\n",
        "df.withColumn ('newDateCol', to_date(lit('14-05-2024'),'dd-MM-yyyy')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97Q_7cd4ji5D",
        "outputId": "6021781d-85b6-4673-9029-3fc8d71b4299"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "| id|todaysDate|\n",
            "+---+----------+\n",
            "|  0|2024-05-14|\n",
            "|  1|2024-05-14|\n",
            "+---+----------+\n",
            "\n",
            "+---+----------+\n",
            "| id| newFormat|\n",
            "+---+----------+\n",
            "|  0|05-14-2024|\n",
            "|  1|05-14-2024|\n",
            "+---+----------+\n",
            "\n",
            "+---+----------+\n",
            "| id|newDateCol|\n",
            "+---+----------+\n",
            "|  0|2024-05-14|\n",
            "|  1|2024-05-14|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#51. datediff(), months_between(), add_months(), date_add(), month(), year() functions in PySpark\n",
        "from pyspark.sql.functions import datediff , months_between, add_months, date_add, year , month\n",
        "df=spark.createDataFrame([('2015-04-08','2015-05-08')],['d1','d2'])\n",
        "df.withColumn('diff',datediff(df.d2,df.d1)).show()\n",
        "df.withColumn('months_between',months_between(df.d2,df.d1)).show()\n",
        "df.withColumn('add_months',add_months(df.d2,4)).show()\n",
        "df.withColumn('submonth',add_months(df.d2,-4)).show()"
      ],
      "metadata": {
        "id": "Mo2LvPoWkqWc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac150ac-c29a-4382-9a8e-3c2049754442"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----+\n",
            "|        d1|        d2|diff|\n",
            "+----------+----------+----+\n",
            "|2015-04-08|2015-05-08|  30|\n",
            "+----------+----------+----+\n",
            "\n",
            "+----------+----------+--------------+\n",
            "|        d1|        d2|months_between|\n",
            "+----------+----------+--------------+\n",
            "|2015-04-08|2015-05-08|           1.0|\n",
            "+----------+----------+--------------+\n",
            "\n",
            "+----------+----------+----------+\n",
            "|        d1|        d2|add_months|\n",
            "+----------+----------+----------+\n",
            "|2015-04-08|2015-05-08|2015-09-08|\n",
            "+----------+----------+----------+\n",
            "\n",
            "+----------+----------+----------+\n",
            "|        d1|        d2|  submonth|\n",
            "+----------+----------+----------+\n",
            "|2015-04-08|2015-05-08|2015-01-08|\n",
            "+----------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('addDate',date_add(df.d2,4)).show()\n",
        "df.withColumn('subDate',date_add(df.d2,-4)).show()\n",
        "df.withColumn('year',year(df.d2)).show()\n",
        "df.withColumn('month',month(df.d2)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcX2nyNqOlyl",
        "outputId": "7d3af229-d54f-45d9-a67e-4b14384923e2"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+\n",
            "|        d1|        d2|   addDate|\n",
            "+----------+----------+----------+\n",
            "|2015-04-08|2015-05-08|2015-05-12|\n",
            "+----------+----------+----------+\n",
            "\n",
            "+----------+----------+----------+\n",
            "|        d1|        d2|   subDate|\n",
            "+----------+----------+----------+\n",
            "|2015-04-08|2015-05-08|2015-05-04|\n",
            "+----------+----------+----------+\n",
            "\n",
            "+----------+----------+----+\n",
            "|        d1|        d2|year|\n",
            "+----------+----------+----+\n",
            "|2015-04-08|2015-05-08|2015|\n",
            "+----------+----------+----+\n",
            "\n",
            "+----------+----------+-----+\n",
            "|        d1|        d2|month|\n",
            "+----------+----------+-----+\n",
            "|2015-04-08|2015-05-08|    5|\n",
            "+----------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#52. Timestamp Functions in PySpark\n",
        "from pyspark.sql.functions import current_timestamp , to_timestamp , lit , hour , minute , second\n",
        "df=spark.range(2)\n",
        "df.show()\n",
        "df1=df.withColumn('timestamp',current_timestamp())\n",
        "df1.show(truncate=False)\n",
        "df1.printSchema()\n",
        "df2=df1.withColumn('toTimestamp',to_timestamp(lit('25.12.2022 06.10.13'), 'dd.MM.yyyy HH.mm.ss'))\n",
        "df2.show(truncate=False)\n",
        "df2.printSchema()\n",
        "df2.select('id',hour(current_timestamp()).alias('hour'),\\\n",
        "           minute(current_timestamp()).alias('minute'),\\\n",
        "           second(current_timestamp()).alias('second')).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5drqlDuxPyxe",
        "outputId": "9dc2c942-c01f-4316-8d65-0b365f415d0e"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "+---+\n",
            "\n",
            "+---+--------------------------+\n",
            "|id |timestamp                 |\n",
            "+---+--------------------------+\n",
            "|0  |2024-05-14 08:47:58.716124|\n",
            "|1  |2024-05-14 08:47:58.716124|\n",
            "+---+--------------------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = false)\n",
            " |-- timestamp: timestamp (nullable = false)\n",
            "\n",
            "+---+-------------------------+-------------------+\n",
            "|id |timestamp                |toTimestamp        |\n",
            "+---+-------------------------+-------------------+\n",
            "|0  |2024-05-14 08:47:58.82591|2022-12-25 06:10:13|\n",
            "|1  |2024-05-14 08:47:58.82591|2022-12-25 06:10:13|\n",
            "+---+-------------------------+-------------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = false)\n",
            " |-- timestamp: timestamp (nullable = false)\n",
            " |-- toTimestamp: timestamp (nullable = true)\n",
            "\n",
            "+---+----+------+------+\n",
            "| id|hour|minute|second|\n",
            "+---+----+------+------+\n",
            "|  0|   8|    47|    59|\n",
            "|  1|   8|    47|    59|\n",
            "+---+----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#53. approx_count_distinct(), avg(), collect_list(), collect_set(), countDistinct(), count()\n",
        "from pyspark.sql.functions import approx_count_distinct, avg, collect_list , collect_set , countDistinct, count\n",
        "\n",
        "simpleData = [('Payeer','HR',4500),\\\n",
        "              ('Fayeer','SDE',1300),\\\n",
        "              ('Yayeer','Manager',4500)]\n",
        "\n",
        "schema= [\"employee_name\",'dep','salary']\n",
        "df=spark.createDataFrame(data=simpleData,schema=schema)\n",
        "df.printSchema()\n",
        "df.show(truncate = False)\n",
        "df.select(approx_count_distinct('salary')).show()\n",
        "df.select(avg('salary')).show()\n",
        "df.select(collect_list('salary')).show()\n",
        "df.select(collect_set('salary')).show()\n",
        "df.select(countDistinct('salary')).show()\n",
        "df.select(count('salary')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c3IUX08SXng",
        "outputId": "4585752d-82b8-4049-ac30-c281f7d8a620"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- dep: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+-------+------+\n",
            "|employee_name|dep    |salary|\n",
            "+-------------+-------+------+\n",
            "|Payeer       |HR     |4500  |\n",
            "|Fayeer       |SDE    |1300  |\n",
            "|Yayeer       |Manager|4500  |\n",
            "+-------------+-------+------+\n",
            "\n",
            "+-----------------------------+\n",
            "|approx_count_distinct(salary)|\n",
            "+-----------------------------+\n",
            "|                            2|\n",
            "+-----------------------------+\n",
            "\n",
            "+------------------+\n",
            "|       avg(salary)|\n",
            "+------------------+\n",
            "|3433.3333333333335|\n",
            "+------------------+\n",
            "\n",
            "+--------------------+\n",
            "|collect_list(salary)|\n",
            "+--------------------+\n",
            "|  [4500, 1300, 4500]|\n",
            "+--------------------+\n",
            "\n",
            "+-------------------+\n",
            "|collect_set(salary)|\n",
            "+-------------------+\n",
            "|       [4500, 1300]|\n",
            "+-------------------+\n",
            "\n",
            "+----------------------+\n",
            "|count(DISTINCT salary)|\n",
            "+----------------------+\n",
            "|                     2|\n",
            "+----------------------+\n",
            "\n",
            "+-------------+\n",
            "|count(salary)|\n",
            "+-------------+\n",
            "|            3|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#54. row_number(), rank(), dense_rank() functions in PySpark\n",
        "from pyspark.sql.functions import row_number , rank , dense_rank\n",
        "from pyspark.sql.window import Window\n",
        "data=[(1,'naheey','M',5000,'IT'),\\\n",
        "      (2,'baheey','F',50100,'IT'),\\\n",
        "      (3,'gaheey','M',52000,'IT'),\\\n",
        "      (4,'taheey','F',53000,'GIT'),\\\n",
        "      (5,'raheey','M',25000,'GIT'),\\\n",
        "      (6,'eaheey','F',15000,'KIT'),\\\n",
        "      (7,'waheey','M',95000,'KIT')]\n",
        "\n",
        "schema=['id','name','gender','salary','dep']\n",
        "df=spark.createDataFrame(data,schema)\n",
        "\n",
        "# df.sort('dep').show()\n",
        "\n",
        "window=Window.partitionBy('dep').orderBy('salary')\n",
        "df.withColumn('rowNumber',row_number().over(window)).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTCJ9g_0asmW",
        "outputId": "ca7341b2-aa02-4831-fee6-7d460c8d64f9"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+------+---+---------+\n",
            "| id|  name|gender|salary|dep|rowNumber|\n",
            "+---+------+------+------+---+---------+\n",
            "|  5|raheey|     M| 25000|GIT|        1|\n",
            "|  4|taheey|     F| 53000|GIT|        2|\n",
            "|  1|naheey|     M|  5000| IT|        1|\n",
            "|  2|baheey|     F| 50100| IT|        2|\n",
            "|  3|gaheey|     M| 52000| IT|        3|\n",
            "|  6|eaheey|     F| 15000|KIT|        1|\n",
            "|  7|waheey|     M| 95000|KIT|        2|\n",
            "+---+------+------+------+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('rank',rank().over(window)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZwkJ9H5eR-V",
        "outputId": "4b8ae48b-a026-4c33-d1b0-c962a27f98f7"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+------+---+----+\n",
            "| id|  name|gender|salary|dep|rank|\n",
            "+---+------+------+------+---+----+\n",
            "|  5|raheey|     M| 25000|GIT|   1|\n",
            "|  4|taheey|     F| 53000|GIT|   2|\n",
            "|  1|naheey|     M|  5000| IT|   1|\n",
            "|  2|baheey|     F| 50100| IT|   2|\n",
            "|  3|gaheey|     M| 52000| IT|   3|\n",
            "|  6|eaheey|     F| 15000|KIT|   1|\n",
            "|  7|waheey|     M| 95000|KIT|   2|\n",
            "+---+------+------+------+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn('denserrank',dense_rank().over(window)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_WGPYjqfSww",
        "outputId": "87559bb7-f10f-4c5d-af02-1e42a212f32d"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+------+---+----------+\n",
            "| id|  name|gender|salary|dep|denserrank|\n",
            "+---+------+------+------+---+----------+\n",
            "|  5|raheey|     M| 25000|GIT|         1|\n",
            "|  4|taheey|     F| 53000|GIT|         2|\n",
            "|  1|naheey|     M|  5000| IT|         1|\n",
            "|  2|baheey|     F| 50100| IT|         2|\n",
            "|  3|gaheey|     M| 52000| IT|         3|\n",
            "|  6|eaheey|     F| 15000|KIT|         1|\n",
            "|  7|waheey|     M| 95000|KIT|         2|\n",
            "+---+------+------+------+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YbJa7Mndfb_t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}